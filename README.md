# pytorch-activation-functions-comparison
- A comparison of common activation functions in PyTorch using MNIST dataset

## Activation functions:
- Relu
- Sigmoid
- Tanh

## Best result: Relu
