# pytorch-activation-functions-comparison
- Comparison of common activation functions in PyTorch using MNIST dataset

Activation functions:
- Relu,
- Sigmoid,
- Tanh.

Best result: Relu
